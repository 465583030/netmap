diff --git a/configure b/configure
index dcaa67c..55ef412 100755
--- a/configure
+++ b/configure
@@ -146,6 +146,7 @@ curl=""
 curses=""
 docs=""
 fdt=""
+netmap=""
 nptl=""
 pixman=""
 sdl=""
@@ -740,6 +741,10 @@ for opt do
   ;;
   --enable-vde) vde="yes"
   ;;
+  --disable-netmap) netmap="no"
+  ;;
+  --enable-netmap) netmap="yes"
+  ;;
   --disable-xen) xen="no"
   ;;
   --enable-xen) xen="yes"
@@ -1117,6 +1122,8 @@ echo "  --disable-uuid           disable uuid support"
 echo "  --enable-uuid            enable uuid support"
 echo "  --disable-vde            disable support for vde network"
 echo "  --enable-vde             enable support for vde network"
+echo "  --disable-netmap         disable support for netmap network"
+echo "  --enable-netmap          enable support for netmap network"
 echo "  --disable-linux-aio      disable Linux AIO support"
 echo "  --enable-linux-aio       enable Linux AIO support"
 echo "  --disable-cap-ng         disable libcap-ng support"
@@ -1939,6 +1946,26 @@ EOF
 fi
 
 ##########################################
+# netmap headers probe
+if test "$netmap" != "no" ; then
+  cat > $TMPC << EOF
+#include <inttypes.h>
+#include <net/if.h>
+#include <net/netmap.h>
+#include <net/netmap_user.h>
+int main(void) { return 0; }
+EOF
+  if compile_prog "" "" ; then
+    netmap=yes
+  else
+    if test "$netmap" = "yes" ; then
+      feature_not_found "netmap"
+    fi
+    netmap=no
+  fi
+fi
+
+##########################################
 # libcap-ng library probe
 if test "$cap_ng" != "no" ; then
   cap_libs="-lcap-ng"
@@ -3364,6 +3391,7 @@ echo "NPTL support      $nptl"
 echo "GUEST_BASE        $guest_base"
 echo "PIE               $pie"
 echo "vde support       $vde"
+echo "netmap support    $netmap"
 echo "Linux AIO support $linux_aio"
 echo "ATTR/XATTR support $attr"
 echo "Install blobs     $blobs"
@@ -3489,6 +3517,9 @@ fi
 if test "$vde" = "yes" ; then
   echo "CONFIG_VDE=y" >> $config_host_mak
 fi
+if test "$netmap" = "yes" ; then
+  echo "CONFIG_NETMAP=y" >> $config_host_mak
+fi
 if test "$cap_ng" = "yes" ; then
   echo "CONFIG_LIBCAP=y" >> $config_host_mak
 fi
diff --git a/exec.c b/exec.c
index a41bcb8..e6ef820 100644
--- a/exec.c
+++ b/exec.c
@@ -2059,6 +2059,35 @@ static void cpu_notify_map_clients(void)
     }
 }
 
+/* Helper function returning the contiguous segment containing
+ * a guest physical address (gpaddr).
+ * Return 0 if not existing, otherwise the segment covers the
+ * guest physical region *gpa_low .. *gpa_high - 1, and the
+ * guest-physical to host-virtual mapping is obtained as
+ *         host_virtual_addr = gp_addr + *g2h_ofs
+ */
+int address_space_mappable(AddressSpace *as, hwaddr gp_addr,
+    uint64_t *gpa_lo, uint64_t *gpa_hi, uint64_t *g2h_ofs)
+{
+    AddressSpaceDispatch *d = as->dispatch;
+    MemoryRegionSection *section;
+    RAMBlock *block;
+
+    section = phys_page_find(d, gp_addr >> TARGET_PAGE_BITS);
+    if (memory_region_is_ram(section->mr) && !section->readonly) {
+        QTAILQ_FOREACH(block, &ram_list.blocks, next) {
+            if (gp_addr - block->offset < block->length) {
+                *gpa_lo = block->offset;
+                *gpa_hi = block->offset + block->length;
+                *g2h_ofs = (uint64_t)block->host - block->offset;
+                return 1;
+            }
+        }
+    }
+    *gpa_lo = *gpa_hi = *g2h_ofs = 0;
+    return 0;    /* cannot map */
+}
+
 /* Map a physical memory region into a host virtual address.
  * May map a subset of the requested range, given by and returned in *plen.
  * May return NULL if resources needed to perform the mapping are exhausted.
diff --git a/hw/e1000.c b/hw/e1000.c
index d6fe815..16ae5de 100644
--- a/hw/e1000.c
+++ b/hw/e1000.c
@@ -35,6 +35,59 @@
 
 #include "e1000_hw.h"
 
+#define MAP_RING        /* map the buffers instead of pci_dma_rw() */
+#define PARAVIRT        /* use paravirtualized driver */
+
+#ifdef PARAVIRT
+/*
+ Support for virtio-like communication.
+ 1. the VMM advertises virtio-like synchronization setting
+    the subvendor id set to 0x1101 (E1000_PARA_SUBDEV)
+
+ 2. the guest allocates the shared command status block (csb) and
+    write its physical address at CSBAL and CSBAH (offsets
+    0x2830 and 0x2834, data is little endian).
+    csb->csb_on enables the mode. If disabled, the device is a
+    regular e1000.
+
+ 3. notifications for tx and rx are exchanged without vm exits
+    if possible. In particular (only mentioning csb mode below):
+
+ TX: host sets host_need_txkick=1 when the I/O thread bh is idle.
+     Guest updates guest_tdt and returns if host_need_txkick == 0,
+     otherwise dues a regular write to the TDT.
+     If the txring runs dry, guest sets guest_need_txkick and retries
+     to recover buffers.
+     Host reacts to writes to the TDT by clearing host_need_txkick
+     and scheduling a thread to do the reads.
+     The thread is kept active until there are packets (with a
+     configurable number of retries). Eventually it sets
+     host_need_txkick=1, does a final check for packets and blocks.
+     An interrupt is generated if guest_need_txkick == 1.
+
+ */
+#define E1000_PARA_SUBDEV 0x1101
+#define E1000_CSBAL       0x02830 /* addresses for the csb */
+#define E1000_CSBAH       0x02834
+struct e1000_csb {
+    /* these are written by the guest */
+    uint32_t guest_tdt;            /* pkt to transmit */
+    uint32_t guest_need_txkick;    /* ran out of tx bufs, request kick */
+    uint32_t guest_need_rxkick;    /* ran out of rx pkts, request kick ? */
+    uint32_t guest_csb_on;         /* enable paravirtual mode */
+    uint32_t guest_rdt;            /* rx buffers available */
+    uint32_t pad[11];
+
+    /* these are (mostly) written by the host */
+    uint32_t host_tdh;             /* shadow registea, mostly unused */
+    uint32_t host_need_txkick;     /* start the iothread */
+    uint32_t host_txcycles_lim;    /* how much to spin before  sleep */
+    uint32_t host_txcycles;        /* counter, but no need to be exported */
+    uint32_t host_rdh;             /* shadow register, mostly unused */
+    uint32_t host_need_rxkick;     /* ??? */
+};
+#endif /* PARAVIRT */
+
 #define E1000_DEBUG
 
 #ifdef E1000_DEBUG
@@ -72,7 +125,9 @@ static int debugflags = DBGBIT(TXERR) | DBGBIT(GENERAL);
  *  E1000_DEV_ID_82544GC_COPPER appears to work; not well tested
  *  Others never tested
  */
-enum { E1000_DEVID = E1000_DEV_ID_82540EM };
+enum { E1000_DEVID = E1000_DEV_ID_82540EM }; // microwire
+//enum { E1000_DEVID = E1000_DEV_ID_82573L }; // eeprom eerd
+// enum { E1000_DEVID = E1000_DEV_ID_82571EB_COPPER }; // eeprom eerd
 
 /*
  * May need to specify additional MAC-to-PHY entries --
@@ -84,6 +139,18 @@ enum {
                    /* default to E1000_DEV_ID_82540EM */	0xc20
 };
 
+/*
+ * map a guest region into a host region
+ * if the pointer is within the region, ofs gives the displacement.
+ * valid = 0 means we should try to map it.
+ */
+struct guest_memreg_map {
+        int      valid;
+        uint64_t lo;
+        uint64_t hi;
+        uint64_t ofs;
+};
+
 typedef struct E1000State_st {
     PCIDevice dev;
     NICState *nic;
@@ -131,6 +198,28 @@ typedef struct E1000State_st {
     } eecd_state;
 
     QEMUTimer *autoneg_timer;
+    QEMUTimer *mit_timer;       /* handle for the timer          */
+    uint32_t  mit_timer_on;     /* mitigation timer active       */
+    uint32_t  mit_cause;        /* pending interrupt cause       */
+    uint32_t  mit_on;           /* mitigation enable             */
+
+    /* when the rxq becomes full, disable input until half empty */
+    uint32_t rxbufs, txbufs, rxq_full;
+#ifdef MAP_RING
+    /* used for map ring */
+    uint64_t txring_phi, rxring_phi;         /* phisical address */
+    struct e1000_tx_desc *txring;
+    struct e1000_rx_desc *rxring;
+    struct guest_memreg_map mbufs;
+#endif /* MAP_RING */
+
+#ifdef PARAVIRT
+    /* used for the communication block */
+    struct e1000_csb *csb;
+    QEMUBH       *tx_bh;
+    uint32_t     tx_count;          /* written in last round */
+    QEMUBH       *rx_bh;
+#endif /* PARAVIRT */
 } E1000State;
 
 #define	defreg(x)	x = (E1000_##x>>2)
@@ -146,8 +235,50 @@ enum {
     defreg(TPR),	defreg(TPT),	defreg(TXDCTL),	defreg(WUFC),
     defreg(RA),		defreg(MTA),	defreg(CRCERRS),defreg(VFTA),
     defreg(VET),
+    defreg(RDTR),       defreg(RADV),   defreg(TADV),   defreg(ITR),
+#ifdef PARAVIRT
+    defreg(CSBAL),      defreg(CSBAH),
+#endif /* PARAVIRT */
 };
 
+#ifdef MAP_RING
+/*
+ * try to extract an mbuf region
+ */
+static const uint8_t *map_mbufs(E1000State *s, hwaddr addr)
+{
+    struct guest_memreg_map *mb = &s->mbufs;
+    uint64_t a = addr;
+    DMAContext *dma;
+
+    for (;;) {
+        if (mb->valid && a >= mb->lo && a < mb->hi) {
+            return (const uint8_t *)(a + mb->ofs);
+        }
+        dma = pci_dma_context(&s->dev);
+        mb->valid = 1;
+
+        D("mapping %p is unset", (void *)addr);
+        if (dma_has_iommu(dma)) {
+            D("iommu range, cannot set");
+            break;
+        }
+        if (!address_space_mappable(dma->as, addr,
+                  &mb->lo, &mb->hi, &mb->ofs)) {
+            D("not mappable, cannot set");
+            break;
+        }
+        D("segment [%p .. %p] delta %p",
+             (void *)mb->lo, (void *)mb->hi, (void *)mb->ofs);
+
+        D("mapping txring correct %p computed %p",
+            s->txring, (void *)(s->txring_phi + mb->ofs));
+    }
+    mb->hi = mb->lo = 0; /* empty mapping */
+    return NULL;
+}
+#endif /* MAP_RING */
+
 static void
 e1000_link_down(E1000State *s)
 {
@@ -378,12 +509,12 @@ set_eecd(E1000State *s, int index, uint32_t val)
     s->eecd_state.old_eecd = val & (E1000_EECD_SK | E1000_EECD_CS |
             E1000_EECD_DI|E1000_EECD_FWE_MASK|E1000_EECD_REQ);
     if (!(E1000_EECD_CS & val))			// CS inactive; nothing to do
-	return;
+        return;
     if (E1000_EECD_CS & (val ^ oldval)) {	// CS rise edge; reset state
-	s->eecd_state.val_in = 0;
-	s->eecd_state.bitnum_in = 0;
-	s->eecd_state.bitnum_out = 0;
-	s->eecd_state.reading = 0;
+        s->eecd_state.val_in = 0;
+        s->eecd_state.bitnum_in = 0;
+        s->eecd_state.bitnum_out = 0;
+        s->eecd_state.reading = 0;
     }
     if (!(E1000_EECD_SK & (val ^ oldval)))	// no clock edge
         return;
@@ -543,7 +674,7 @@ process_tx_desc(E1000State *s, struct e1000_tx_desc *dp)
     uint32_t txd_lower = le32_to_cpu(dp->lower.data);
     uint32_t dtype = txd_lower & (E1000_TXD_CMD_DEXT | E1000_TXD_DTYP_D);
     unsigned int split_size = txd_lower & 0xffff, bytes, sz, op;
-    unsigned int msh = 0xfffff, hdr = 0;
+    unsigned int hdr = 0;
     uint64_t addr;
     struct e1000_context_desc *xp = (struct e1000_context_desc *)dp;
     struct e1000_tx *tp = &s->tx;
@@ -575,7 +706,7 @@ process_tx_desc(E1000State *s, struct e1000_tx_desc *dp)
         }
         tp->cptse = ( txd_lower & E1000_TXD_CMD_TSE ) ? 1 : 0;
     } else {
-        // legacy descriptor
+        /* legacy descriptor, max len 16288 bytes */
         tp->cptse = 0;
     }
 
@@ -587,11 +718,30 @@ process_tx_desc(E1000State *s, struct e1000_tx_desc *dp)
         cpu_to_be16wu((uint16_t *)(tp->vlan_header + 2),
                       le16_to_cpu(dp->upper.fields.special));
     }
-        
+
     addr = le64_to_cpu(dp->buffer_addr);
+
+#ifdef MAP_RING
+    if (!tp->tse && !tp->cptse && tp->size == 0 &&
+        !tp->vlan_needed && !tp->sum_needed &&
+        (txd_lower & E1000_TXD_CMD_EOP)) {
+            const uint8_t *x = map_mbufs(s, addr);
+        if (x) {
+            /* XXX optimization for netmap */
+            e1000_send_packet(s, x, split_size);
+            tp->tso_frames = 0;
+            tp->sum_needed = 0;
+            tp->vlan_needed = 0;
+            tp->size = 0;
+            tp->cptse = 0;
+            return ;
+        }
+    }
+#endif /* MAP_RING */
+
     if (tp->tse && tp->cptse) {
         hdr = tp->hdr_len;
-        msh = hdr + tp->mss;
+        unsigned int msh = hdr + tp->mss;
         do {
             bytes = split_size;
             if (tp->size + bytes > msh)
@@ -639,12 +789,16 @@ txdesc_writeback(E1000State *s, dma_addr_t base, struct e1000_tx_desc *dp)
     txd_upper = (le32_to_cpu(dp->upper.data) | E1000_TXD_STAT_DD) &
                 ~(E1000_TXD_STAT_EC | E1000_TXD_STAT_LC | E1000_TXD_STAT_TU);
     dp->upper.data = cpu_to_le32(txd_upper);
+#ifdef MAP_RING
+    s->txring[s->mac_reg[TDH]].upper = dp->upper;
+#else /* !MAP_RING */
     pci_dma_write(&s->dev, base + ((char *)&dp->upper - (char *)dp),
                   &dp->upper, sizeof(dp->upper));
+#endif /* !MAP_RING */
     return E1000_ICR_TXDW;
 }
 
-static uint64_t tx_desc_base(E1000State *s)
+static inline uint64_t tx_desc_base(E1000State *s)
 {
     uint64_t bah = s->mac_reg[TDBAH];
     uint64_t bal = s->mac_reg[TDBAL] & ~0xf;
@@ -652,6 +806,73 @@ static uint64_t tx_desc_base(E1000State *s)
     return (bah << 32) + bal;
 }
 
+/* helper function, 0 means the value is not set */
+static inline void
+mit_update_delay(uint32_t *curr, uint32_t value)
+{
+    if (value && (*curr == 0 || value < *curr)) {
+        *curr = value;
+    }
+}
+
+/*
+ * If necessary, rearm the timer and post an interrupt.
+ * Called at the end of tx/rx routines (mit_timer_on == 0),
+ * and when the timer fires (mit_timer_on == 1).
+ * We provide a partial implementation of interrupt mitigation,
+ * emulating only RADV, TADV and ITR (lower 16 bits, 1024ns units for
+ * RADV and TADV, 256ns units for ITR). RDTR is only used to enable RADV;
+ * relative timers based on TIDV and RDTR are not implemented.
+ */
+static void
+mit_rearm_and_int(void *opaque)
+{
+    E1000State *s = opaque;
+    uint32_t mit_delay = 0;
+
+    /*
+     * Clear the flag. It is only set when the callback fires,
+     * and we need to clear it anyways.
+     */
+    s->mit_timer_on = 0;
+    if (s->mit_cause == 0) { /* no events pending, we are done */
+        return;
+    }
+    /*
+     * Compute the next mitigation delay according to pending interrupts
+     * and the current values of RADV (provided RDTR!=0), TADV and ITR.
+     * Then rearm the timer.
+     */
+    if (s->mit_cause & (E1000_ICR_TXQE | E1000_ICR_TXDW)) {
+        mit_update_delay(&mit_delay, s->mac_reg[TADV] * 4);
+    }
+    if (s->mac_reg[RDTR] && (s->mit_cause & E1000_ICS_RXT0)) {
+        mit_update_delay(&mit_delay, s->mac_reg[RADV] * 4);
+    }
+    mit_update_delay(&mit_delay, s->mac_reg[ITR]);
+
+    if (mit_delay) {
+        s->mit_timer_on = 1;
+        qemu_mod_timer(s->mit_timer,
+                qemu_get_clock_ns(vm_clock) + mit_delay * 256);
+    }
+
+    set_ics(s, 0, s->mit_cause);
+    s->mit_cause = 0;
+}
+
+static void
+mit_set_ics(E1000State *s, uint32_t cause)
+{
+    if (s->mit_on == 0) {
+        set_ics(s, 0, cause);
+        return;
+    }
+    s->mit_cause |= cause;
+    if (!s->mit_timer_on)
+        mit_rearm_and_int(s);
+}
+
 static void
 start_xmit(E1000State *s)
 {
@@ -664,10 +885,56 @@ start_xmit(E1000State *s)
         return;
     }
 
+#ifdef MAP_RING
+    base = tx_desc_base(s);
+    if (base != s->txring_phi) {
+        hwaddr desclen = s->mac_reg[TDLEN];
+        s->txring_phi = base;
+        s->txring = address_space_map(pci_dma_context(&s->dev)->as,
+              base, &desclen, 0 /* is_write */);
+        D("region size is %ld", desclen);
+    }
+#endif /* MAP_RING */
+
+#ifdef PARAVIRT
+    /* hlim prevents staying here for too long */
+    uint32_t hlim = s->mac_reg[TDLEN] / sizeof(desc) / 2;
+    uint32_t csb_mode = s->csb && s->csb->guest_csb_on;
+    s->tx_count = 0;
+    for (;;) {
+        if (csb_mode) {
+            if (s->mac_reg[TDH] == s->mac_reg[TDT]) {
+                /* we ran dry, exchange some notifications */
+                smp_mb(); /* read from guest ? */
+                s->mac_reg[TDT] = s->csb->guest_tdt;
+                tdh_start = s->csb->host_tdh = s->mac_reg[TDH];
+            }
+            if (s->tx_count > hlim || s->mac_reg[TDH] == s->mac_reg[TDT]) {
+                /* still dry, we are done */
+                s->csb->host_tdh = s->mac_reg[TDH];
+                if (s->tx_count > 50) {
+                    ND("sent %d in this iteration", s->tx_count);
+                }
+                smp_mb();
+                if (s->csb->guest_need_txkick) {
+                    mit_set_ics(s, cause);
+                }
+                return;
+            }
+        } else if (s->mac_reg[TDH] == s->mac_reg[TDT]) {
+            break;
+        }
+        s->tx_count++;
+#else /* !PARAVIRT */
     while (s->mac_reg[TDH] != s->mac_reg[TDT]) {
+#endif /* PARAVIRT */
+#ifdef MAP_RING
+        desc = s->txring[s->mac_reg[TDH]];
+#else /* !MAP_RING */
         base = tx_desc_base(s) +
                sizeof(struct e1000_tx_desc) * s->mac_reg[TDH];
         pci_dma_read(&s->dev, base, &desc, sizeof(desc));
+#endif /* MAP_RING */
 
         DBGOUT(TX, "index %d: %p : %x %x\n", s->mac_reg[TDH],
                (void *)(intptr_t)desc.buffer_addr, desc.lower.data,
@@ -689,7 +956,7 @@ start_xmit(E1000State *s)
             break;
         }
     }
-    set_ics(s, 0, cause);
+    mit_set_ics(s, cause);
 }
 
 static int
@@ -764,6 +1031,34 @@ e1000_set_link_status(NetClientState *nc)
 static bool e1000_has_rxbufs(E1000State *s, size_t total_size)
 {
     int bufs;
+#ifdef PARAVIRT
+again:
+    if (s->csb && s->csb->guest_csb_on) {
+        smp_mb();
+        s->mac_reg[RDT] = s->csb->guest_rdt;
+    }
+    bufs = s->mac_reg[RDT] - s->mac_reg[RDH];
+
+    if (bufs < 0) {
+        bufs += s->rxbufs;
+    }
+#if 0
+    if (s->rxq_full && bufs < s->rxbufs / 2) {
+        return false; /* hysteresis */
+    }
+#endif
+    s->rxq_full = (total_size > bufs * s->rxbuf_size);
+    if (s->csb && s->csb->guest_csb_on) {
+        if (!s->rxq_full) {
+	    s->csb->host_need_rxkick = 0;
+	} else if (!s->csb->host_need_rxkick) {
+	    s->csb->host_need_rxkick = 1;
+	    goto again;
+	}
+    }
+    return !s->rxq_full;
+#else /* !PARAVIRT */
+
     /* Fast-path short packets */
     if (total_size <= s->rxbuf_size) {
         return s->mac_reg[RDH] != s->mac_reg[RDT];
@@ -777,6 +1072,7 @@ static bool e1000_has_rxbufs(E1000State *s, size_t total_size)
         return false;
     }
     return total_size <= bufs * s->rxbuf_size;
+#endif /* !PARAVIRT */
 }
 
 static int
@@ -788,7 +1084,7 @@ e1000_can_receive(NetClientState *nc)
         (s->mac_reg[RCTL] & E1000_RCTL_EN) && e1000_has_rxbufs(s, 1);
 }
 
-static uint64_t rx_desc_base(E1000State *s)
+static inline uint64_t rx_desc_base(E1000State *s)
 {
     uint64_t bah = s->mac_reg[RDBAH];
     uint64_t bal = s->mac_reg[RDBAL] & ~0xf;
@@ -846,6 +1142,13 @@ e1000_receive(NetClientState *nc, const uint8_t *buf, size_t size)
         size -= 4;
     }
 
+#ifdef PARAVIRT
+    if (s->csb && s->csb->guest_csb_on) {
+        smp_mb();
+        s->mac_reg[RDT] = s->csb->guest_rdt;
+    }
+#endif /* PARAVIRT */
+
     rdh_start = s->mac_reg[RDH];
     desc_offset = 0;
     total_size = size + fcs_len(s);
@@ -853,13 +1156,26 @@ e1000_receive(NetClientState *nc, const uint8_t *buf, size_t size)
             set_ics(s, 0, E1000_ICS_RXO);
             return -1;
     }
+#ifdef MAP_RING
+    base = rx_desc_base(s);
+    if (base != s->rxring_phi) {
+        hwaddr desclen = s->mac_reg[RDLEN];
+        s->rxring_phi = base;
+        s->rxring = address_space_map(pci_dma_context(&s->dev)->as,
+                base, &desclen, 0 /* is_write */);
+    }
+#endif /* MAP_RING */
     do {
         desc_size = total_size - desc_offset;
         if (desc_size > s->rxbuf_size) {
             desc_size = s->rxbuf_size;
         }
         base = rx_desc_base(s) + sizeof(desc) * s->mac_reg[RDH];
+#ifdef MAP_RING
+        desc = s->rxring[s->mac_reg[RDH]];
+#else /* !MAP_RING */
         pci_dma_read(&s->dev, base, &desc, sizeof(desc));
+#endif /* !MAP_RING */
         desc.special = vlan_special;
         desc.status |= (vlan_status | E1000_RXD_STAT_DD);
         if (desc.buffer_addr) {
@@ -883,7 +1199,12 @@ e1000_receive(NetClientState *nc, const uint8_t *buf, size_t size)
         } else { // as per intel docs; skip descriptors with null buf addr
             DBGOUT(RX, "Null RX descriptor!!\n");
         }
+#ifdef MAP_RING
+        s->rxring[s->mac_reg[RDH]] = desc;
+        /* XXX a barrier ? */
+#else
         pci_dma_write(&s->dev, base, &desc, sizeof(desc));
+#endif /* !MAP_RING */
 
         if (++s->mac_reg[RDH] * sizeof(desc) >= s->mac_reg[RDLEN])
             s->mac_reg[RDH] = 0;
@@ -914,7 +1235,16 @@ e1000_receive(NetClientState *nc, const uint8_t *buf, size_t size)
         s->rxbuf_min_shift)
         n |= E1000_ICS_RXDMT0;
 
-    set_ics(s, 0, n);
+#ifdef PARAVIRT
+    // XXX in csb mode, if the guest does not need kick, we are done.
+    if (s->csb && s->csb->guest_csb_on) {
+	if (!s->csb->guest_need_rxkick) {
+	    ND("guest_need_rxkick off, not kicking");
+	    return size;
+        }
+    }
+#endif /* PARAVIRT */
+    mit_set_ics(s, n);
 
     return size;
 }
@@ -960,6 +1290,49 @@ mac_writereg(E1000State *s, int index, uint32_t val)
     s->mac_reg[index] = val;
 }
 
+
+#ifdef PARAVIRT
+static void
+set_32bit(E1000State *s, int index, uint32_t val)
+{
+    s->mac_reg[index] = val;
+    if (index == CSBAH || index == CSBAL) {
+        hwaddr desclen = 4096;
+        hwaddr base = ((uint64_t)s->mac_reg[CSBAH] << 32) | s->mac_reg[CSBAL];
+        s->csb = address_space_map(pci_dma_context(&s->dev)->as,
+              base, &desclen, 0 /* is_write */);
+    }
+}
+
+static void
+e1000_tx_bh(void *opaque)
+{
+    E1000State *s = opaque;
+    struct e1000_csb *csb = s->csb;
+
+    ND("starting tdt %d sent %d in prev.round ", csb->guest_tdt, s->tx_count);
+    s->mac_reg[TDT] = csb->guest_tdt;
+    start_xmit(s);
+    csb->host_txcycles = (s->tx_count > 0) ? 0 : csb->host_txcycles+1;
+    if (csb->host_txcycles >= csb->host_txcycles_lim) {
+        /* prepare to sleep, with race avoidance */
+        csb->host_txcycles = 0;
+        csb->host_need_txkick = 1;
+	ND("tx bh going to sleep, set txkick");
+        smp_mb();
+        /* XXX read tdt */
+        s->mac_reg[TDT] = csb->guest_tdt;
+        if (s->mac_reg[TDH] != s->mac_reg[TDT]) {
+	    ND("tx bh race avoidance, clear txkick");
+            csb->host_need_txkick = 0;
+        }
+    }
+    if (csb->host_need_txkick == 0) {
+        qemu_bh_schedule(s->tx_bh);
+    }
+}
+#endif /* PARAVIRT */
+
 static void
 set_rdt(E1000State *s, int index, uint32_t val)
 {
@@ -979,6 +1352,12 @@ static void
 set_dlen(E1000State *s, int index, uint32_t val)
 {
     s->mac_reg[index] = val & 0xfff80;
+    if (index == RDLEN) {
+        s->rxbufs = s->mac_reg[index] / sizeof(struct e1000_rx_desc);
+        s->rxq_full = 0;
+    } else {
+        s->txbufs = s->mac_reg[index] / sizeof(struct e1000_tx_desc);
+    }
 }
 
 static void
@@ -986,6 +1365,16 @@ set_tctl(E1000State *s, int index, uint32_t val)
 {
     s->mac_reg[index] = val;
     s->mac_reg[TDT] &= 0xffff;
+#ifdef PARAVIRT
+    if (s->csb && s->csb->guest_csb_on) {
+	ND("kick accepted tdt %d guest-tdt %d",
+		s->mac_reg[TDT], s->csb->guest_tdt);
+        s->csb->host_need_txkick = 0; /* XXX could be done by the guest */
+        smp_mb(); /* XXX do we care ? */
+        qemu_bh_schedule(s->tx_bh);
+        return;
+    }
+#endif /* PARAVIRT */
     start_xmit(s);
 }
 
@@ -1019,6 +1408,10 @@ static uint32_t (*macreg_readops[])(E1000State *, int) = {
     getreg(RDH),	getreg(RDT),	getreg(VET),	getreg(ICS),
     getreg(TDBAL),	getreg(TDBAH),	getreg(RDBAH),	getreg(RDBAL),
     getreg(TDLEN),	getreg(RDLEN),
+    getreg(RDTR),       getreg(RADV),   getreg(TADV),   getreg(ITR),
+#ifdef PARAVIRT
+    getreg(CSBAL),      getreg(CSBAH),
+#endif /* PARAVIRT */
 
     [TOTH] = mac_read_clr8,	[TORH] = mac_read_clr8,	[GPRC] = mac_read_clr4,
     [GPTC] = mac_read_clr4,	[TPR] = mac_read_clr4,	[TPT] = mac_read_clr4,
@@ -1035,6 +1428,11 @@ static void (*macreg_writeops[])(E1000State *, int, uint32_t) = {
     putreg(PBA),	putreg(EERD),	putreg(SWSM),	putreg(WUFC),
     putreg(TDBAL),	putreg(TDBAH),	putreg(TXDCTL),	putreg(RDBAH),
     putreg(RDBAL),	putreg(LEDCTL), putreg(VET),
+#ifdef PARAVIRT
+    [CSBAL] = set_32bit, [CSBAH] = set_32bit,
+#endif /* PARAVIRT */
+    [RDTR] = set_16bit, [RADV] = set_16bit,     [TADV] = set_16bit,
+    [ITR] = set_16bit,
     [TDLEN] = set_dlen,	[RDLEN] = set_dlen,	[TCTL] = set_tctl,
     [TDT] = set_tctl,	[MDIC] = set_mdic,	[ICS] = set_ics,
     [TDH] = set_16bit,	[RDH] = set_16bit,	[RDT] = set_rdt,
@@ -1332,6 +1730,13 @@ static int pci_e1000_init(PCIDevice *pci_dev)
 
     d->autoneg_timer = qemu_new_timer_ms(vm_clock, e1000_autoneg_timer, d);
 
+    d->mit_cause = 0;
+    d->mit_timer_on = 0;
+    d->mit_timer = qemu_new_timer_ns(vm_clock, mit_rearm_and_int, d);
+
+#ifdef PARAVIRT
+    d->tx_bh = qemu_bh_new(e1000_tx_bh, d);
+#endif /* PARAVIRT */
     return 0;
 }
 
@@ -1343,6 +1748,7 @@ static void qdev_e1000_reset(DeviceState *dev)
 
 static Property e1000_properties[] = {
     DEFINE_NIC_PROPERTIES(E1000State, conf),
+    DEFINE_PROP_UINT32("mit_on", E1000State, mit_on, 5),
     DEFINE_PROP_END_OF_LIST(),
 };
 
@@ -1356,6 +1762,9 @@ static void e1000_class_init(ObjectClass *klass, void *data)
     k->romfile = "pxe-e1000.rom";
     k->vendor_id = PCI_VENDOR_ID_INTEL;
     k->device_id = E1000_DEVID;
+#ifdef PARAVIRT
+    k->subsystem_id = E1000_PARA_SUBDEV;
+#endif /* PARAVIRT */
     k->revision = 0x03;
     k->class_id = PCI_CLASS_NETWORK_ETHERNET;
     dc->desc = "Intel Gigabit Ethernet";
diff --git a/hw/virtio-net.c b/hw/virtio-net.c
index 573c669..5389088 100644
--- a/hw/virtio-net.c
+++ b/hw/virtio-net.c
@@ -21,6 +21,8 @@
 #include "virtio-net.h"
 #include "vhost_net.h"
 
+#define VIRTIO_Q_SLOTS	256	// 256
+
 #define VIRTIO_NET_VM_VERSION    11
 
 #define MAC_TABLE_ENTRIES    64
@@ -49,6 +51,7 @@ typedef struct VirtIONet
     NICState *nic;
     uint32_t tx_timeout;
     int32_t tx_burst;
+    int32_t tx_retries; // XXX lr
     uint32_t has_vnet_hdr;
     size_t host_hdr_len;
     size_t guest_hdr_len;
@@ -1062,7 +1065,10 @@ static void virtio_net_tx_bh(void *opaque)
 
     /* If we flush a full burst of packets, assume there are
      * more coming and immediately reschedule */
-    if (ret >= n->tx_burst) {
+    if (ret == 0)
+	n->tx_retries++;
+    // if (ret >= n->tx_burst) {
+    if (n->tx_retries < 20) {
         qemu_bh_schedule(q->tx_bh);
         q->tx_waiting = 1;
         return;
@@ -1076,6 +1082,8 @@ static void virtio_net_tx_bh(void *opaque)
         virtio_queue_set_notification(q->tx_vq, 0);
         qemu_bh_schedule(q->tx_bh);
         q->tx_waiting = 1;
+    } else {
+	n->tx_retries = 0;
     }
 }
 
@@ -1091,16 +1099,16 @@ static void virtio_net_set_multiqueue(VirtIONet *n, int multiqueue, int ctrl)
     }
 
     for (i = 1; i < max; i++) {
-        n->vqs[i].rx_vq = virtio_add_queue(vdev, 256, virtio_net_handle_rx);
+        n->vqs[i].rx_vq = virtio_add_queue(vdev, VIRTIO_Q_SLOTS, virtio_net_handle_rx);
         if (n->vqs[i].tx_timer) {
             n->vqs[i].tx_vq =
-                virtio_add_queue(vdev, 256, virtio_net_handle_tx_timer);
+                virtio_add_queue(vdev, VIRTIO_Q_SLOTS, virtio_net_handle_tx_timer);
             n->vqs[i].tx_timer = qemu_new_timer_ns(vm_clock,
                                                    virtio_net_tx_timer,
                                                    &n->vqs[i]);
         } else {
             n->vqs[i].tx_vq =
-                virtio_add_queue(vdev, 256, virtio_net_handle_tx_bh);
+                virtio_add_queue(vdev, VIRTIO_Q_SLOTS, virtio_net_handle_tx_bh);
             n->vqs[i].tx_bh = qemu_bh_new(virtio_net_tx_bh, &n->vqs[i]);
         }
 
@@ -1326,7 +1334,7 @@ VirtIODevice *virtio_net_init(DeviceState *dev, NICConf *conf,
     n->vdev.set_status = virtio_net_set_status;
     n->vdev.guest_notifier_mask = virtio_net_guest_notifier_mask;
     n->vdev.guest_notifier_pending = virtio_net_guest_notifier_pending;
-    n->vqs[0].rx_vq = virtio_add_queue(&n->vdev, 256, virtio_net_handle_rx);
+    n->vqs[0].rx_vq = virtio_add_queue(&n->vdev, VIRTIO_Q_SLOTS, virtio_net_handle_rx);
     n->max_queues = conf->queues;
     n->curr_queues = 1;
     n->vqs[0].n = n;
@@ -1340,12 +1348,12 @@ VirtIODevice *virtio_net_init(DeviceState *dev, NICConf *conf,
     }
 
     if (net->tx && !strcmp(net->tx, "timer")) {
-        n->vqs[0].tx_vq = virtio_add_queue(&n->vdev, 256,
+        n->vqs[0].tx_vq = virtio_add_queue(&n->vdev, VIRTIO_Q_SLOTS,
                                            virtio_net_handle_tx_timer);
         n->vqs[0].tx_timer = qemu_new_timer_ns(vm_clock, virtio_net_tx_timer,
                                                &n->vqs[0]);
     } else {
-        n->vqs[0].tx_vq = virtio_add_queue(&n->vdev, 256,
+        n->vqs[0].tx_vq = virtio_add_queue(&n->vdev, VIRTIO_Q_SLOTS,
                                            virtio_net_handle_tx_bh);
         n->vqs[0].tx_bh = qemu_bh_new(virtio_net_tx_bh, &n->vqs[0]);
     }
diff --git a/include/exec/memory.h b/include/exec/memory.h
index 2322732..c8f68de 100644
--- a/include/exec/memory.h
+++ b/include/exec/memory.h
@@ -833,6 +833,23 @@ void address_space_init(AddressSpace *as, MemoryRegion *root);
 void address_space_destroy(AddressSpace *as);
 
 /**
+ * address_space_mappable: return region containing a guest address.
+ *
+ * If the guest physical address is mappable in host virtual memory,
+ * the function returns the containing region for which the
+ * mapping is valid, and the offset to be added to the gpa
+ * to generate a host virtual address.
+ *
+ * @as: #AddressSpace to be accessed
+ * @addr: address within that address space
+ * @lo: pointer to the initial address in the range
+ * @hi: pointer after the final address in the range
+ * @ofs: pointer to the delta between the two addresses
+ */
+int address_space_mappable(AddressSpace *as, hwaddr addr, uint64_t *lo,
+        uint64_t *hi, uint64_t *ofs);
+
+/**
  * address_space_rw: read from or write to an address space.
  *
  * @as: #AddressSpace to be accessed
diff --git a/include/net/net.h b/include/net/net.h
index 43a045e..20d3f22 100644
--- a/include/net/net.h
+++ b/include/net/net.h
@@ -11,6 +11,33 @@
 
 #define MAX_QUEUE_NUM 1024
 
+#ifndef ND
+#define ND(fd, ...)    /* debugging */
+#define D(format, ...)                                          \
+        do {                                                    \
+                struct timeval __xxts;                          \
+                gettimeofday(&__xxts, NULL);                    \
+                printf("%03d.%06d %s [%d] " format "\n",        \
+                (int)__xxts.tv_sec % 1000, (int)__xxts.tv_usec, \
+                __func__, __LINE__, ##__VA_ARGS__);         \
+        } while (0)
+
+/* rate limited, lps indicates how many per second */
+#define RD(lps, format, ...)                                    \
+        do {                                                    \
+                static int t0, __cnt;                           \
+                struct timeval __xxts;                          \
+                gettimeofday(&__xxts, NULL);                    \
+                if (t0 != __xxts.tv_sec) {                      \
+                        t0 = __xxts.tv_sec;                     \
+                        __cnt = 0;                              \
+                }                                               \
+                if (__cnt++ < lps) {                            \
+                        D(format, ##__VA_ARGS__);               \
+                }                                               \
+        } while (0)
+#endif
+
 struct MACAddr {
     uint8_t a[6];
 };
diff --git a/net/Makefile.objs b/net/Makefile.objs
index a08cd14..8cb9f2f 100644
--- a/net/Makefile.objs
+++ b/net/Makefile.objs
@@ -10,3 +10,4 @@ common-obj-$(CONFIG_AIX) += tap-aix.o
 common-obj-$(CONFIG_HAIKU) += tap-haiku.o
 common-obj-$(CONFIG_SLIRP) += slirp.o
 common-obj-$(CONFIG_VDE) += vde.o
+common-obj-$(CONFIG_NETMAP) += netmap.o
diff --git a/net/clients.h b/net/clients.h
index 7793294..952d076 100644
--- a/net/clients.h
+++ b/net/clients.h
@@ -52,4 +52,8 @@ int net_init_vde(const NetClientOptions *opts, const char *name,
                  NetClientState *peer);
 #endif
 
+#ifdef CONFIG_NETMAP
+int net_init_netmap(const NetClientOptions *opts, const char *name,
+                    NetClientState *peer);
+#endif
 #endif /* QEMU_NET_CLIENTS_H */
diff --git a/net/hub.c b/net/hub.c
index a24c9d1..df32074 100644
--- a/net/hub.c
+++ b/net/hub.c
@@ -338,3 +338,17 @@ void net_hub_check_clients(void)
         }
     }
 }
+
+bool net_hub_flush(NetClientState *nc)
+{
+    NetHubPort *port;
+    NetHubPort *source_port = DO_UPCAST(NetHubPort, nc, nc);
+    int ret = 0;
+
+    QLIST_FOREACH(port, &source_port->hub->ports, next) {
+        if (port != source_port) {
+            ret += qemu_net_queue_flush(port->nc.send_queue);
+        }
+    }
+    return ret ? true : false;
+}
diff --git a/net/hub.h b/net/hub.h
index 583ada8..a625eff 100644
--- a/net/hub.h
+++ b/net/hub.h
@@ -21,5 +21,6 @@ NetClientState *net_hub_add_port(int hub_id, const char *name);
 NetClientState *net_hub_find_client_by_name(int hub_id, const char *name);
 void net_hub_info(Monitor *mon);
 void net_hub_check_clients(void);
+bool net_hub_flush(NetClientState *nc);
 
 #endif /* NET_HUB_H */
diff --git a/net/net.c b/net/net.c
index be03a8d..3dceb29 100644
--- a/net/net.c
+++ b/net/net.c
@@ -441,6 +441,12 @@ void qemu_flush_queued_packets(NetClientState *nc)
 {
     nc->receive_disabled = 0;
 
+    if (nc->peer && nc->peer->info->type == NET_CLIENT_OPTIONS_KIND_HUBPORT) {
+        if (net_hub_flush(nc->peer)) {
+            qemu_notify_event();
+        }
+        return;
+    }
     if (qemu_net_queue_flush(nc->send_queue)) {
         /* We emptied the queue successfully, signal to the IO thread to repoll
          * the file descriptor (for tap, for example).
@@ -480,7 +486,8 @@ ssize_t qemu_send_packet_async(NetClientState *sender,
 
 void qemu_send_packet(NetClientState *nc, const uint8_t *buf, int size)
 {
-    qemu_send_packet_async(nc, buf, size, NULL);
+    qemu_send_packet_async_with_flags(nc, QEMU_NET_PACKET_FLAG_NONE,
+         buf, size, NULL);
 }
 
 ssize_t qemu_send_packet_raw(NetClientState *nc, const uint8_t *buf, int size)
@@ -723,6 +730,9 @@ static int (* const net_client_init_fun[NET_CLIENT_OPTIONS_KIND_MAX])(
         [NET_CLIENT_OPTIONS_KIND_BRIDGE]    = net_init_bridge,
 #endif
         [NET_CLIENT_OPTIONS_KIND_HUBPORT]   = net_init_hubport,
+#ifdef CONFIG_NETMAP
+        [NET_CLIENT_OPTIONS_KIND_NETMAP]    = net_init_netmap,
+#endif
 };
 
 
diff --git a/net/netmap.c b/net/netmap.c
new file mode 100644
index 0000000..794a7f4
--- /dev/null
+++ b/net/netmap.c
@@ -0,0 +1,364 @@
+/*
+ * netmap access for qemu
+ *
+ * Copyright (c) 2012-2013 Luigi Rizzo
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include "config-host.h"
+
+/* note paths are different for -head and 1.3 */
+#include "net/net.h"
+#include "clients.h"
+#include "sysemu/sysemu.h"
+#include "qemu-common.h"
+#include "qemu/error-report.h"
+
+#include <sys/ioctl.h>
+#include <net/if.h>
+#include <sys/mman.h>
+#include <net/netmap.h>
+#include <net/netmap_user.h>
+
+#ifndef ND
+#define ND(fd, ...)        /* debugging */
+#define D(format, ...)                                          \
+        do {                                                    \
+                struct timeval __xxts;                          \
+                gettimeofday(&__xxts, NULL);                    \
+                printf("%03d.%06d %s [%d] " format "\n",        \
+                (int)__xxts.tv_sec % 1000, (int)__xxts.tv_usec, \
+                __func__, __LINE__, ##__VA_ARGS__);         \
+        } while (0)
+
+/* rate limited, lps indicates how many per second */
+#define RD(lps, format, ...)                                    \
+        do {                                                    \
+                static int t0, __cnt;                           \
+                struct timeval __xxts;                          \
+                gettimeofday(&__xxts, NULL);                    \
+                if (t0 != __xxts.tv_sec) {                      \
+                        t0 = __xxts.tv_sec;                     \
+                        __cnt = 0;                              \
+                }                                               \
+                if (__cnt++ < lps) {                            \
+                        D(format, ##__VA_ARGS__);               \
+                }                                               \
+        } while (0)
+#endif
+
+
+/*
+ * private netmap device info
+ */
+struct netmap_state {
+    int                 fd;
+    int                 memsize;
+    void                *mem;
+    struct netmap_if    *nifp;
+    struct netmap_ring  *rx;
+    struct netmap_ring  *tx;
+    char                fdname[128];        /* normally /dev/netmap */
+    char                ifname[128];        /* maybe the nmreq here ? */
+};
+
+struct nm_state {
+    NetClientState      nc;
+    struct netmap_state me;
+    unsigned int        read_poll;
+    unsigned int        write_poll;
+};
+
+#ifndef __FreeBSD__
+#define pkt_copy bcopy
+#else
+/* a fast copy routine only for multiples of 64 bytes, non overlapped. */
+static inline void
+pkt_copy(const void *_src, void *_dst, int l)
+{
+    const uint64_t *src = _src;
+    uint64_t *dst = _dst;
+#define likely(x)       __builtin_expect(!!(x), 1)
+#define unlikely(x)       __builtin_expect(!!(x), 0)
+    if (unlikely(l >= 1024)) {
+        bcopy(src, dst, l);
+        return;
+    }
+    for (; l > 0; l -= 64) {
+        *dst++ = *src++;
+        *dst++ = *src++;
+        *dst++ = *src++;
+        *dst++ = *src++;
+        *dst++ = *src++;
+        *dst++ = *src++;
+        *dst++ = *src++;
+        *dst++ = *src++;
+    }
+}
+#endif /* __FreeBSD__ */
+
+
+/*
+ * open a netmap device. We assume there is only one queue
+ * (which is the case for the VALE bridge).
+ */
+static int netmap_open(struct netmap_state *me)
+{
+    int fd, err;
+    size_t l;
+    struct nmreq req;
+
+    me->fd = fd = open(me->fdname, O_RDWR);
+    if (fd < 0) {
+        error_report("Unable to open netmap device '%s'", me->fdname);
+        return -1;
+    }
+    bzero(&req, sizeof(req));
+    pstrcpy(req.nr_name, sizeof(req.nr_name), me->ifname);
+    req.nr_ringid = 0;
+    req.nr_version = NETMAP_API;
+    err = ioctl(fd, NIOCGINFO, &req);
+    if (err) {
+        error_report("cannot get info on %s", me->ifname);
+        goto error;
+    }
+    l = me->memsize = req.nr_memsize;
+    err = ioctl(fd, NIOCREGIF, &req);
+    if (err) {
+        error_report("Unable to register %s", me->ifname);
+        goto error;
+    }
+
+    me->mem = mmap(0, l, PROT_WRITE | PROT_READ, MAP_SHARED, fd, 0);
+    if (me->mem == MAP_FAILED) {
+        error_report("Unable to mmap");
+        me->mem = NULL;
+        goto error;
+    }
+
+    me->nifp = NETMAP_IF(me->mem, req.nr_offset);
+    me->tx = NETMAP_TXRING(me->nifp, 0);
+    me->rx = NETMAP_RXRING(me->nifp, 0);
+    return 0;
+
+error:
+    close(me->fd);
+    return -1;
+}
+
+/* XXX do we need the can-send routine ? */
+static int netmap_can_send(void *opaque)
+{
+    struct nm_state *s = opaque;
+
+    return qemu_can_send_packet(&s->nc);
+}
+
+static void netmap_send(void *opaque);
+static void netmap_writable(void *opaque);
+
+/*
+ * set the handlers for the device
+ */
+static void netmap_update_fd_handler(struct nm_state *s)
+{
+    qemu_set_fd_handler2(s->me.fd,
+                         s->read_poll  ? netmap_can_send : NULL,
+                         s->read_poll  ? netmap_send     : NULL,
+                         s->write_poll ? netmap_writable : NULL,
+                         s);
+}
+
+/* update the read handler */
+static void netmap_read_poll(struct nm_state *s, bool enable)
+{
+    if (s->read_poll != enable) { /* do nothing if not changed */
+        s->read_poll = enable;
+        netmap_update_fd_handler(s);
+    }
+}
+
+/* update the write handler */
+static void netmap_write_poll(struct nm_state *s, bool enable)
+{
+    if (s->write_poll != enable) {
+        s->write_poll = enable;
+        netmap_update_fd_handler(s);
+    }
+}
+
+static void netmap_poll(NetClientState *nc, bool enable)
+{
+    struct nm_state *s = DO_UPCAST(struct nm_state, nc, nc);
+
+    if (s->read_poll != enable || s->write_poll != enable) {
+        s->read_poll = enable;
+        s->read_poll = enable;
+        netmap_update_fd_handler(s);
+    }
+}
+
+/*
+ * the fd_write() callback, invoked if the fd is marked as
+ * writable after a poll. Reset the handler and flush any
+ * buffered packets.
+ */
+static void netmap_writable(void *opaque)
+{
+    struct nm_state *s = opaque;
+
+    netmap_write_poll(s, false);
+    qemu_flush_queued_packets(&s->nc);
+}
+
+/*
+ * new data guest --> backend
+ */
+static ssize_t netmap_receive_raw(NetClientState *nc,
+      const uint8_t *buf, size_t size)
+{
+    struct nm_state *s = DO_UPCAST(struct nm_state, nc, nc);
+    struct netmap_ring *ring = s->me.tx;
+
+    if (size > ring->nr_buf_size) {
+        RD(5, "drop packet of size %d > %d", (int)size, ring->nr_buf_size);
+        return size;
+    }
+
+    if (ring) {
+        /* request an early notification to avoid running dry */
+        if (ring->avail < ring->num_slots / 2 && s->write_poll == false) {
+            netmap_write_poll(s, true);
+        }
+        if (ring->avail == 0) { /* cannot write */
+            return 0;
+        }
+        uint32_t i = ring->cur;
+        uint32_t idx = ring->slot[i].buf_idx;
+        uint8_t *dst = (uint8_t *)NETMAP_BUF(ring, idx);
+
+        ring->slot[i].len = size;
+        pkt_copy(buf, dst, size);
+        ring->cur = NETMAP_RING_NEXT(ring, i);
+        ring->avail--;
+    }
+    return size;
+}
+
+/* complete a previous send (backend --> guest), enable the fd_read callback */
+static void netmap_send_completed(NetClientState *nc, ssize_t len)
+{
+    struct nm_state *s = DO_UPCAST(struct nm_state, nc, nc);
+
+    netmap_read_poll(s, true);
+}
+
+/*
+ * netmap_send: backend -> guest
+ * there is traffic available from the network, try to send it up.
+ */
+static void netmap_send(void *opaque)
+{
+    struct nm_state *s = opaque;
+    struct netmap_ring *ring = s->me.rx;
+
+    /* only check ring->avail, let the packet be queued
+     * with qemu_send_packet_async() if needed
+     * XXX until we fix the propagation on the bridge we need to stop early
+     */
+    while (ring->avail > 0 && qemu_can_send_packet(&s->nc)) {
+        uint32_t i = ring->cur;
+        uint32_t idx = ring->slot[i].buf_idx;
+        uint8_t *src = (u_char *)NETMAP_BUF(ring, idx);
+        int size = ring->slot[i].len;
+
+        ring->cur = NETMAP_RING_NEXT(ring, i);
+        ring->avail--;
+        size = qemu_send_packet_async(&s->nc, src, size, netmap_send_completed);
+        if (size == 0) {
+            /* the guest does not receive anymore. Packet is queued, stop
+             * reading from the backend until netmap_send_completed()
+             */
+            netmap_read_poll(s, false);
+            return;
+        }
+    }
+    netmap_read_poll(s, true); /* probably useless. */
+}
+
+
+/* flush and close */
+static void netmap_cleanup(NetClientState *nc)
+{
+    struct nm_state *s = DO_UPCAST(struct nm_state, nc, nc);
+
+    qemu_purge_queued_packets(nc);
+
+    netmap_poll(nc, false);
+    munmap(s->me.mem, s->me.memsize);
+    close(s->me.fd);
+
+    s->me.fd = -1;
+}
+
+
+
+/* fd support */
+
+static NetClientInfo net_netmap_info = {
+    .type = NET_CLIENT_OPTIONS_KIND_NETMAP,
+    .size = sizeof(struct nm_state),
+    .receive = netmap_receive_raw,
+#if 0 /* not implemented */
+    .receive_raw = netmap_receive_raw,
+    .receive_iov = netmap_receive_iov,
+#endif
+    .poll = netmap_poll,
+    .cleanup = netmap_cleanup,
+};
+
+/* the external calls */
+
+/*
+ * ... -net netmap,ifname="..."
+ */
+int net_init_netmap(const NetClientOptions *opts,
+        const char *name, NetClientState *peer)
+{
+    const NetdevNetmapOptions *netmap_opts = opts->netmap;
+    NetClientState *nc;
+    struct netmap_state me;
+    struct nm_state *s;
+
+    pstrcpy(me.fdname, sizeof(me.fdname), name ? name : "/dev/netmap");
+    /* set default name for the port if not supplied */
+    pstrcpy(me.ifname, sizeof(me.ifname),
+        netmap_opts->has_ifname ? netmap_opts->ifname : "vale0");
+    if (netmap_open(&me)) {
+        return -1;
+    }
+    /* create the object -- XXX use name or ifname ? */
+    nc = qemu_new_net_client(&net_netmap_info, peer, "netmap", name);
+    s = DO_UPCAST(struct nm_state, nc, nc);
+    s->me = me;
+    netmap_read_poll(s, true); /* initially only poll for reads. */
+
+    return 0;
+}
diff --git a/net/queue.c b/net/queue.c
index 6eaf5b6..859d02a 100644
--- a/net/queue.c
+++ b/net/queue.c
@@ -50,6 +50,8 @@ struct NetPacket {
 
 struct NetQueue {
     void *opaque;
+    uint32_t nq_maxlen;
+    uint32_t nq_count;
 
     QTAILQ_HEAD(packets, NetPacket) packets;
 
@@ -63,6 +65,8 @@ NetQueue *qemu_new_net_queue(void *opaque)
     queue = g_malloc0(sizeof(NetQueue));
 
     queue->opaque = opaque;
+    queue->nq_maxlen = 10000;
+    queue->nq_count = 0;
 
     QTAILQ_INIT(&queue->packets);
 
@@ -92,6 +96,9 @@ static void qemu_net_queue_append(NetQueue *queue,
 {
     NetPacket *packet;
 
+    if (queue->nq_count >= queue->nq_maxlen && !sent_cb) {
+        return; /* drop if queue full and no callback */
+    }
     packet = g_malloc(sizeof(NetPacket) + size);
     packet->sender = sender;
     packet->flags = flags;
@@ -99,6 +106,7 @@ static void qemu_net_queue_append(NetQueue *queue,
     packet->sent_cb = sent_cb;
     memcpy(packet->data, buf, size);
 
+    queue->nq_count++;
     QTAILQ_INSERT_TAIL(&queue->packets, packet, entry);
 }
 
@@ -113,6 +121,9 @@ static void qemu_net_queue_append_iov(NetQueue *queue,
     size_t max_len = 0;
     int i;
 
+    if (queue->nq_count >= queue->nq_maxlen && !sent_cb) {
+        return; /* drop if queue full and no callback */
+    }
     for (i = 0; i < iovcnt; i++) {
         max_len += iov[i].iov_len;
     }
@@ -130,6 +141,7 @@ static void qemu_net_queue_append_iov(NetQueue *queue,
         packet->size += len;
     }
 
+    queue->nq_count++;
     QTAILQ_INSERT_TAIL(&queue->packets, packet, entry);
 }
 
@@ -220,6 +232,7 @@ void qemu_net_queue_purge(NetQueue *queue, NetClientState *from)
     QTAILQ_FOREACH_SAFE(packet, &queue->packets, entry, next) {
         if (packet->sender == from) {
             QTAILQ_REMOVE(&queue->packets, packet, entry);
+            queue->nq_count--;
             g_free(packet);
         }
     }
@@ -233,6 +246,7 @@ bool qemu_net_queue_flush(NetQueue *queue)
 
         packet = QTAILQ_FIRST(&queue->packets);
         QTAILQ_REMOVE(&queue->packets, packet, entry);
+        queue->nq_count--;
 
         ret = qemu_net_queue_deliver(queue,
                                      packet->sender,
@@ -240,6 +254,7 @@ bool qemu_net_queue_flush(NetQueue *queue)
                                      packet->data,
                                      packet->size);
         if (ret == 0) {
+            queue->nq_count++;
             QTAILQ_INSERT_HEAD(&queue->packets, packet, entry);
             return false;
         }
diff --git a/qapi-schema.json b/qapi-schema.json
index cd7ea25..b6316e1 100644
--- a/qapi-schema.json
+++ b/qapi-schema.json
@@ -2641,6 +2641,11 @@
   'data': {
     'hubid':     'int32' } }
 
+{ 'type': 'NetdevNetmapOptions',
+  'data': {
+    '*ifname':     'str' } }
+
+
 ##
 # @NetClientOptions
 #
@@ -2658,7 +2663,8 @@
     'vde':      'NetdevVdeOptions',
     'dump':     'NetdevDumpOptions',
     'bridge':   'NetdevBridgeOptions',
-    'hubport':  'NetdevHubPortOptions' } }
+    'hubport':  'NetdevHubPortOptions',
+    'netmap':   'NetdevNetmapOptions' } }
 
 ##
 # @NetLegacy
