--- mellanox/mlx4/en_cq.c	2012-09-11 20:50:55.982624673 -0700
+++ ./mellanox/mlx4/en_cq.c	2012-09-25 19:59:14.163852149 -0700
@@ -37,6 +37,10 @@
 
 #include "mlx4_en.h"
 
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+#include <mlx4_netmap_linux.h>        /* extern stuff */
+#endif /* CONFIG_NETMAP */
+
 static void mlx4_en_cq_event(struct mlx4_cq *cq, enum mlx4_event event)
 {
 	return;
--- mellanox/mlx4/en_netdev.c	2012-09-11 20:50:55.982624673 -0700
+++ ./mellanox/mlx4/en_netdev.c	2012-09-23 17:46:55.489660556 -0700
@@ -48,6 +48,39 @@
 #include "mlx4_en.h"
 #include "en_port.h"
 
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+/*
+ * This driver is split in multiple small files.
+ * The main device descriptor has type struct mlx4_en_priv *priv;
+ * and we attach to the device in mlx4_en_init_netdev()
+ * (do port numbers start from 1 ?)
+ *
+ * The reconfig routine is in mlx4_en_start_port() (also here)
+ * which is called on a mlx4_en_restart() (watchdog), open and set-mtu.
+ *
+ *	priv->num_frags				??
+ *	DS_SIZE					??
+ *		apparently each rx desc is followed by frag.descriptors
+ *		and the rx desc is rounded up to a power of 2.
+ *
+ *   Receive code is in en_rx.c
+ *	priv->rx_ring_num			number of rx rings
+ *	rxr = prov->rx_ring[ring_ind]		rx ring descriptor
+ *	rxr->size				number of slots
+ *	rxr->prod				producer
+ *	   probably written into a mmio reg at *rxr->wqres.db.db
+ *	   trimmed to 16 bits.
+ *
+ *	Rx init routine:
+ *		mlx4_en_activate_rx_rings()
+ *		  mlx4_en_init_rx_desc()
+ *   Transmit code is in en_tx.c
+ */
+
+#define NETMAP_MLX4_MAIN
+#include <mlx4_netmap_linux.h>        /* extern stuff */
+#endif /* CONFIG_NETMAP */
+
 int mlx4_en_setup_tc(struct net_device *dev, u8 up)
 {
 	if (up != MLX4_EN_NUM_UP)
@@ -1042,6 +1075,9 @@ int mlx4_en_start_port(struct net_device
 		/* Set initial ownership of all Tx TXBBs to SW (1) */
 		for (j = 0; j < tx_ring->buf_size; j += STAMP_STRIDE)
 			*((u32 *) (tx_ring->buf + j)) = 0xffffffff;
+#ifdef DEV_NETMAP
+		mlx4_netmap_tx_config(priv, i);
+#endif /* DEV_NETMAP */
 		++tx_index;
 	}
 
@@ -1639,6 +1675,9 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	en_warn(priv, "Using %d RX rings\n", prof->rx_ring_num);
 
 	queue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);
+#ifdef DEV_NETMAP
+	mlx4_netmap_attach(priv);
+#endif /* DEV_NETMAP */
 	return 0;
 
 out:
--- mellanox/mlx4/en_rx.c	2012-09-11 20:50:55.982624673 -0700
+++ ./mellanox/mlx4/en_rx.c	2012-09-23 17:46:55.489660556 -0700
@@ -41,6 +41,9 @@
 
 #include "mlx4_en.h"
 
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+#include <mlx4_netmap_linux.h>
+#endif /* !DEV_NETMAP */
 
 static int mlx4_en_alloc_frag(struct mlx4_en_priv *priv,
 			      struct mlx4_en_rx_desc *rx_desc,
@@ -365,6 +368,9 @@ int mlx4_en_activate_rx_rings(struct mlx
 		ring = &priv->rx_ring[ring_ind];
 
 		ring->size_mask = ring->actual_size - 1;
+#ifdef DEV_NETMAP
+		mlx4_netmap_rx_config(priv, ring_ind);
+#endif /* DEV_NETMAP */
 		mlx4_en_update_rx_prod_db(ring);
 	}
 
--- mellanox/mlx4/en_tx.c	2012-09-11 20:50:55.982624673 -0700
+++ ./mellanox/mlx4/en_tx.c	2012-09-25 21:14:09.275218854 -0700
@@ -55,6 +55,10 @@ MODULE_PARM_DESC(inline_thold, "threshol
 
 static u32 hashrnd __read_mostly;
 
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+#include <mlx4_netmap_linux.h>        /* extern stuff */
+#endif /* CONFIG_NETMAP */
+
 int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv,
 			   struct mlx4_en_tx_ring *ring, u32 size,
 			   u16 stride)
@@ -396,6 +400,17 @@ void mlx4_en_tx_irq(struct mlx4_cq *mcq)
 
 	if (!spin_trylock(&ring->comp_lock))
 		return;
+#ifdef DEV_NETMAP
+	static int cnt = 0;
+	RD(5,"XXXXXX-------XXXXXXXXXXX-------- tx-irq %d count %d", (int)cq->ring, cnt++);
+	if (netmap_tx_irq(cq->dev, cq->ring)) {
+		RD(5, "wakeup queue %d", cq->ring);
+	} else {
+		RD(5, "XXXXXXXXX  tx_irq %d unexpected, ignoring", cq->ring);
+	}
+	spin_unlock(&ring->comp_lock);
+	return;
+#endif /* DEV_NETMAP */
 	mlx4_en_process_tx_cq(cq->dev, cq);
 	mod_timer(&cq->timer, jiffies + 1);
 	spin_unlock(&ring->comp_lock);
@@ -714,6 +729,7 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff
 	index = ring->prod & ring->size_mask;
 	bf_index = ring->prod;
 
+ND(5, "sending packet ring %d size %d", tx_ind, skb->len);
 	/* See if we have enough space for whole descriptor TXBB for setting
 	 * SW ownership on next descriptor; if not, use a bounce buffer. */
 	if (likely(index + nr_txbb <= ring->size))
@@ -799,6 +815,7 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff
 	data += skb_shinfo(skb)->nr_frags + tx_info->linear - 1;
 
 	if (!is_inline(skb, &fragptr)) {
+ND(5, "ring %d packet not inline, size %d frags %d", tx_ind, skb_headlen(skb), skb_shinfo(skb)->nr_frags);
 		/* Map fragments */
 		for (i = skb_shinfo(skb)->nr_frags - 1; i >= 0; i--) {
 			frag = &skb_shinfo(skb)->frags[i];
@@ -823,6 +840,7 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff
 		}
 		tx_info->inl = 0;
 	} else {
+ND(5, "ring %d packet inline, size %d frags %d", tx_ind, skb_headlen(skb), skb_shinfo(skb)->nr_frags);
 		build_inline_wqe(tx_desc, skb, real_size, &vlan_tag, tx_ind, fragptr);
 		tx_info->inl = 1;
 	}
@@ -861,6 +879,7 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff
 		wmb();
 		iowrite32be(ring->doorbell_qpn, ring->bf.uar->map + MLX4_SEND_DOORBELL);
 	}
+	ND(3, "dumped desc %d ring %d", ring->prod + mlx4_tx_desc_dump(tx_desc), tx_ind);
 
 	/* Poll CQ here */
 	mlx4_en_xmit_poll(priv, tx_ind);
